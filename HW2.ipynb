{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rcp125/On-My-Terms/blob/master/HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMVnoRA3hDfO",
        "colab_type": "text"
      },
      "source": [
        "# Homework 2\n",
        "CS 425/525 Brain-Inspired Computing Spring-2020\n",
        "\n",
        "---\n",
        "\n",
        "**Name:** Dhruvil Patel\n",
        "**NetId:** dhp68\n",
        "**RUId:** 171004047\n",
        "\n",
        "**Name:** Rahul Patel\n",
        "**NetId:** rcp125\n",
        "**RUId:** 185005393 \n",
        "\n",
        "**Name:** Soham Palande\n",
        "**NetId:** ssp215\n",
        "**RUId:** 187006241 \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apg31BAWiTbF",
        "colab_type": "code",
        "outputId": "bf457533-3bf2-4563-a592-6194b82006e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\"\"\" Imports \"\"\"\n",
        "import math\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "from random import uniform\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "from itertools import tee, product\n",
        "print(str(datetime.now())+' -- Installed dependencies!')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-05-04 23:36:25.526602 -- Installed dependencies!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sf3yvSQ3t7YN",
        "colab_type": "text"
      },
      "source": [
        "## Problem 1: XOR\n",
        "### (1.3) 3-layer SNN Using **LIF**\n",
        "\n",
        "![LIF Neuron Model](https://i.ibb.co/TMkmYnX/lif-model.png)\n",
        "\n",
        "Source Credit: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5557947/pdf/41598_2017_Article_7418.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4hAVOqhuZm8",
        "colab_type": "code",
        "outputId": "b30d2431-64a7-4023-e4b3-72e485f79ba8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        }
      },
      "source": [
        "class LIF_Neuron():\n",
        "\n",
        "  # (i_layer) & (i_neuron) for 3-layer SNN\n",
        "  def __init__(self, capacitance=2.5, resistance=2, rest_voltage=0, spike_voltage=75, i_layer=-1, i_neuron=-1):\n",
        "    self.spikes = []\n",
        "    self.values = []\n",
        "    self.voltage = rest_voltage\n",
        "    self.R_m = resistance\n",
        "    self.C_m = capacitance\n",
        "    self.tau = self.R_m * self.C_m\n",
        "    self.V_max = spike_voltage\n",
        "    self.V_rest = rest_voltage\n",
        "    self.i_layer = i_layer\n",
        "    self.i_neuron = i_neuron\n",
        "    self.last_spike = -1\n",
        "    self.values.append(rest_voltage)\n",
        "\n",
        "  # expects (inputCurrent & currentTime)\n",
        "  def reccur_func(self, i_current, time):\n",
        "    self.voltage += i_current / self.C_m\n",
        "    self.voltage -= self.voltage / self.tau\n",
        "    self.values.append(min(self.voltage, self.V_max))\n",
        "    # If below condition met ? spike : reset\n",
        "    if self.voltage > self.V_max:\n",
        "      self.spikes.append(time)\n",
        "      self.last_spike = time\n",
        "      self.voltage = self.V_rest\n",
        "      self.values.append(self.V_max * 2.5)\n",
        "      self.values.append(0)\n",
        "      self.values.append(self.voltage)\n",
        "      return 1\n",
        "    elif self.voltage < self.V_rest:\n",
        "      self.voltage = self.V_rest\n",
        "      self.values.append(self.voltage)\n",
        "    return 0\n",
        "\n",
        "  \"\"\" Supporting functions for SNN\"\"\"\n",
        "  \n",
        "  def get_last_spike(self):\n",
        "    return self.last_spike\n",
        "\n",
        "  def has_neuron_spiked(self):\n",
        "    return not self.last_spike == -1\n",
        "\n",
        "  def persist_spike(self, curr_time):\n",
        "    return self.last_spike == curr_time\n",
        "\n",
        "  def reset_last_spike(self):\n",
        "    self.last_spike = -1\n",
        "\n",
        "  def reset_all(self):\n",
        "    self.last_spike = -1\n",
        "    self.spikes = []\n",
        "\n",
        "\n",
        "\"\"\" TEST LIF NEURON \"\"\"\n",
        "RES = 1\n",
        "CAP = 10\n",
        "R_VOLTAGE = 0\n",
        "S_VOLTAGE = 75\n",
        "TOTAL_TIME = 100\n",
        "\n",
        "x_values, y_values = ([], [])\n",
        "for current in range(1, 300):\n",
        "    x_values.append(current)\n",
        "    neuron = LIF_Neuron(CAP, RES, R_VOLTAGE, S_VOLTAGE)\n",
        "    for j in range(TOTAL_TIME):\n",
        "        neuron.reccur_func(current, j)\n",
        "    spike_freq = neuron.values.count(S_VOLTAGE * 2.5)\n",
        "    y_values.append(spike_freq / TOTAL_TIME)\n",
        "plt.plot(x_values, y_values)\n",
        "plt.xlabel('Input Current')\n",
        "plt.ylabel('Firing Rate')\n",
        "plt.title('LIF Neuron: Input Current vs. Firing Rate')\n",
        "print('LIF Simulator')\n",
        "print('=======================================')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LIF Simulator\n",
            "=======================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAfqfGUCcAKn",
        "colab_type": "text"
      },
      "source": [
        "### (1.4) Hebbian Learning\n",
        "In Hebbian learning, a synapse between two neurons is strengthened when the neurons on either side of the synapse (input and output) have highly correlated outputs. In essence, when an input neuron fires, if it frequently leads to the firing of the output neuron, the synapse is strengthened. Following the analogy to an artificial system, the tap weight is increased with high correlation between two sequential neurons.\n",
        "\n",
        "### Oja's Rule\n",
        "![Oja's Rule 1](https://i.ibb.co/bHb2VYy/Screen-Shot-2020-05-03-at-2-20-27-PM.png)\n",
        "![Oja's Rule 2](https://i.ibb.co/rHqHXJ3/Screen-Shot-2020-05-03-at-2-20-50-PM.png)\n",
        "Source credit: https://en.wikipedia.org/wiki/Oja%27s_rule\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_uzBuCIcF9D",
        "colab_type": "code",
        "outputId": "74662e05-8f24-43a3-dc3d-09cc9f1e8600",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 971
        }
      },
      "source": [
        "\"\"\" Helper Function \"\"\"\n",
        "def p_correlation(iterable):\n",
        "    a, b = tee(iterable)\n",
        "    next(b)\n",
        "    return zip(a, b)\n",
        "\n",
        "class HebbSNN:\n",
        "  def __init__(self):\n",
        "    # Neurons_per_layer (input, hidden, output)\n",
        "    self.snn_layers = [2, 4, 1]\n",
        "    self.total_time = 50\n",
        "    self.trained = defaultdict()\n",
        "    self.layers = [[LIF_Neuron() for i in range(n)] for n in self.snn_layers]\n",
        "    self.synapses = [np.random.rand(len1, len2) for len1, len2 in p_correlation(self.snn_layers)]\n",
        "\n",
        "  # 3-layer SNN\n",
        "  def train(self):\n",
        "    inps = [12, 16]\n",
        "    otpts = [5, 8]\n",
        "    if len(self.trained) <= 0:\n",
        "        self.reset_for_all()\n",
        "        n_spikes, n_win = 0, 1\n",
        "        output_neuron = self.layers[-1][0]\n",
        "        all_inputs = map(lambda rate_pair: [self.fine_tuning(rate)\n",
        "                                            for rate in rate_pair], list(product(inps, inps)))\n",
        "\n",
        "        print('===== STARTED TRAINING =====')\n",
        "        for i_current in all_inputs:\n",
        "            target_rate = otpts[0] if i_current[0] == i_current[1] else otpts[1]\n",
        "            for i_window in range(n_win):\n",
        "                self.reset_for_all()\n",
        "                spikes = defaultdict(int)\n",
        "                for c_time in range(1, 1 + self.total_time):\n",
        "                    for i_layer in range(len(self.layers)):\n",
        "                        for i_neuron in range(len(self.layers[i_layer])):\n",
        "                            neuron = self.layers[i_layer][i_neuron]\n",
        "                            if i_layer == 0:\n",
        "                                curr = i_current[i_neuron]\n",
        "                            else:\n",
        "                                curr = self.the_sum(c_time, i_layer, i_neuron)\n",
        "                            if neuron is output_neuron:\n",
        "                                if c_time % (self.total_time / target_rate) == 0:\n",
        "                                    curr += 1000\n",
        "                                else:\n",
        "                                    curr = 0\n",
        "                            if neuron.reccur_func(curr, c_time) == 1: spikes[neuron] += 1\n",
        "\n",
        "                self.update_weights(c_time, spikes)\n",
        "\n",
        "            for syn in snn.synapses:\n",
        "                for r in syn:\n",
        "                    print('\\t{}'.format(r))\n",
        "            print('(0,0): {}'.format(self.classify_xor([0, 0])))\n",
        "            print('(0,1): {}'.format(self.classify_xor([0, 1])))\n",
        "            print('(1,0): {}'.format(self.classify_xor([1, 0])))\n",
        "            print('(1,1): {}'.format(self.classify_xor([1, 1])))\n",
        "\n",
        "        print('***** SUCCESSFULLY TRAINED! *****')\n",
        "        self.reset_for_all()\n",
        "        self.trained['HEBB-XOR'] = True\n",
        "\n",
        "  # Using Oja's Rule\n",
        "  def update_weights(self, c_time, spikes):\n",
        "    for i_c_synapse in range(len(self.synapses)):\n",
        "      c_synapse = self.synapses[i_c_synapse]\n",
        "      pre_layer, post_layer = self.layers[i_c_synapse:i_c_synapse + 2]\n",
        "      for _post in post_layer:\n",
        "          for _pre in pre_layer:\n",
        "              r, c = pre_layer.index(_pre), post_layer.index(_post)\n",
        "              # learning_rate: 7\n",
        "              c_synapse[r][c] += (spikes[_pre] * spikes[_post] - c_synapse[r][c] * spikes[\n",
        "                  _pre] ** 2) * 1. / 7\n",
        "\n",
        "      \"\"\" Normalize using (Oja's rule) \"\"\"\n",
        "      col_sums = defaultdict(int)\n",
        "      for r in range(len(c_synapse)):\n",
        "          for c in range(len(c_synapse[r])):\n",
        "              col_sums[c] += c_synapse[r][c]\n",
        "      for r in range(len(c_synapse)):\n",
        "          for c in range(len(c_synapse[r])):\n",
        "              c_synapse[r][c] /= col_sums[c]\n",
        "\n",
        "  # classify inputs for trained model\n",
        "  def classify(self, inputs):\n",
        "      self.reset_for_all()\n",
        "      return self.classify_xor(inputs)\n",
        "\n",
        "  def classify_xor(self, inputs):\n",
        "    inps = [12, 16]\n",
        "    i_conv = {i: self.fine_tuning(inps[i]) for i in range(len(inputs))}\n",
        "    n_spikes, n_win = 0, 20\n",
        "    t_threshold = self.total_time * n_win\n",
        "    for c_time in range(1, t_threshold):\n",
        "        for i_layer in range(len(self.layers)):\n",
        "            for i_neuron in range(len(self.layers[i_layer])):\n",
        "                neuron = self.layers[i_layer][i_neuron]\n",
        "                if i_layer == 0:\n",
        "                    curr = i_conv[inputs[i_neuron]]\n",
        "                else:\n",
        "                    curr = self.the_sum(c_time, i_layer, i_neuron)\n",
        "                spike = neuron.reccur_func(curr, c_time)\n",
        "                if i_layer == 2:\n",
        "                    n_spikes += spike\n",
        "    return n_spikes / n_win\n",
        "\n",
        "  # the sum of weights (Oja's Rule)\n",
        "  def the_sum(self, c_time, i_layer, i_neuron):\n",
        "    current = 0\n",
        "    c_layer = self.layers[i_layer]\n",
        "    p_layer = self.layers[i_layer - 1]\n",
        "    c_synapse = self.synapses[i_layer - 1]\n",
        "    for i_p_neuron in range(len(p_layer)):\n",
        "        p_neuron = p_layer[i_p_neuron]\n",
        "        p_spike = p_neuron.last_spike\n",
        "        if p_spike != -1:\n",
        "            weight = c_synapse[i_p_neuron][i_neuron]\n",
        "            new_weight = weight * LIF_Neuron().V_max * math.e ** (-(c_time - p_spike) / 3)\n",
        "            current += new_weight\n",
        "    return current\n",
        "\n",
        "  def fine_tuning(self, rate):\n",
        "    for current in range(0, 1000, 1):\n",
        "        neuron = LIF_Neuron()\n",
        "        spikes = 0\n",
        "        for c_time in range(1, self.total_time + 1):\n",
        "            if neuron.reccur_func(current / 10., c_time) == 1:\n",
        "                spikes += 1\n",
        "        if spikes == rate:\n",
        "            return current / 10.\n",
        "        if spikes == self.total_time:\n",
        "            break\n",
        "    return -1\n",
        "\n",
        "  # For every neurons in the network\n",
        "  def reset_for_all(self):\n",
        "      for layer in self.layers:\n",
        "          for neuron in layer:\n",
        "              neuron.last_spike = -1\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" TEST 3-LAYER SNN (HEBBIAN LEARNING|OJA's RULE) \"\"\"\n",
        "\n",
        "snn = HebbSNN()\n",
        "snn.train()\n",
        "print('===== CLASSIFIED RESULTS =====')\n",
        "for synapse in snn.synapses:\n",
        "    for row in synapse:\n",
        "        print('\\t{}'.format(row))\n",
        "for k in product([0, 1], [0, 1]):\n",
        "    print('{}: {}'.format(k, snn.classify(k)))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===== STARTED TRAINING =====\n",
            "\t[0.72825758 0.38241376 0.39053408 0.03831419]\n",
            "\t[0.27174242 0.61758624 0.60946592 0.96168581]\n",
            "\t[0.10626621]\n",
            "\t[0.04434493]\n",
            "\t[0.16349427]\n",
            "\t[0.68589459]\n",
            "(0,0): 0.0\n",
            "(0,1): 0.05\n",
            "(1,0): 0.05\n",
            "(1,1): 0.0\n",
            "\t[ 0.85710292 -0.09330857 -0.08083395 -0.65158201]\n",
            "\t[0.14289708 1.09330857 1.08083395 1.65158201]\n",
            "\t[0.45703396]\n",
            "\t[0.5970709]\n",
            "\t[0.45790985]\n",
            "\t[-0.51201471]\n",
            "(0,0): 0.0\n",
            "(0,1): 0.0\n",
            "(1,0): 0.05\n",
            "(1,1): 0.45\n",
            "\t[  1.45704497  -2.3027686   -2.15892138 -11.48098153]\n",
            "\t[-0.45704497  3.3027686   3.15892138 12.48098153]\n",
            "\t[0.11588813]\n",
            "\t[0.17162857]\n",
            "\t[0.25347415]\n",
            "\t[0.45900915]\n",
            "(0,0): 3.1\n",
            "(0,1): 4.35\n",
            "(1,0): 49.55\n",
            "(1,1): 0.45\n",
            "\t[  3.17757526  -7.34145372  -6.93900477 -33.01982474]\n",
            "\t[-2.17757526  8.34145372  7.93900477 34.01982474]\n",
            "\t[0.27943919]\n",
            "\t[0.26720348]\n",
            "\t[0.24923738]\n",
            "\t[0.20411994]\n",
            "(0,0): 0.0\n",
            "(0,1): 2.1\n",
            "(1,0): 2.1\n",
            "(1,1): 5.15\n",
            "***** SUCCESSFULLY TRAINED! *****\n",
            "===== CLASSIFIED RESULTS =====\n",
            "\t[  3.17757526  -7.34145372  -6.93900477 -33.01982474]\n",
            "\t[-2.17757526  8.34145372  7.93900477 34.01982474]\n",
            "\t[0.27943919]\n",
            "\t[0.26720348]\n",
            "\t[0.24923738]\n",
            "\t[0.20411994]\n",
            "(0, 0): 4.65\n",
            "(0, 1): 2.05\n",
            "(1, 0): 2.05\n",
            "(1, 1): 4.15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFrUT162uG4f",
        "colab_type": "text"
      },
      "source": [
        "### (1.5) Spike Timing Dependent Plasticity [STDP]\n",
        "![Main Idea of STDP](https://i.ibb.co/6mzprx7/Screen-Shot-2020-05-04-at-11-46-19-AM.png)\n",
        "![Mathematical Equations](https://i.ibb.co/FsgJx4P/Screen-Shot-2020-05-04-at-11-50-55-AM.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPWrynk4uXL4",
        "colab_type": "code",
        "outputId": "7150c9f0-123d-4cc5-cba7-f8b6aea6d59c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\"\"\" Helper Function \"\"\"\n",
        "def p_correlation(itr):\n",
        "    a, b = tee(itr)\n",
        "    next(b)\n",
        "    return zip(a, b)\n",
        "\n",
        "class Synapse:\n",
        "  def __init__(self, pre_synp, post_synp):\n",
        "    self.pre_synp = pre_synp\n",
        "    self.post_synp = post_synp\n",
        "    self.x_j = 0\n",
        "    self.y_i = 0\n",
        "    self.weight = uniform(0, 1)\n",
        "\n",
        "  @staticmethod\n",
        "  def persist_spike(neuron, time):\n",
        "      return neuron.last_spike == time\n",
        "\n",
        "  def update_weights(self, time):\n",
        "      self.x_j += -self.x_j / 4 + self.persist_spike(self.pre_synp, time)\n",
        "      self.y_i += -self.y_i / 4 + self.persist_spike(self.post_synp, time)\n",
        "      self.weight += (-self.weight / 4) * self.y_i * self.persist_spike(self.pre_synp, time) + (self.weight / 4) * self.x_j * self.persist_spike(self.post_synp, time)\n",
        "\n",
        "\n",
        "class STDPSnn:\n",
        "  def __init__(self):\n",
        "    # Neurons_per_layer (input, hidden, output)\n",
        "    self.snn_layers = [2, 4, 1]\n",
        "    self.trained = defaultdict()\n",
        "    self.layers = [[LIF_Neuron(i_layer=i_layer, i_neuron=i_neuron)\n",
        "                        for i_neuron in range(self.snn_layers[i_layer])]\n",
        "                       for i_layer in range(len(self.snn_layers))]\n",
        "    self.synapses = [self.create_synap_layer(pre_layer, post_layer)\n",
        "                      for pre_layer, post_layer in p_correlation(self.layers)]\n",
        "\n",
        "  @staticmethod\n",
        "  def normalize():\n",
        "    k_sum = defaultdict(float)\n",
        "    for s_layer in snn.synapses:\n",
        "        for pre, post_syn in s_layer.items():\n",
        "            for post, synapse in post_syn.items():\n",
        "                k_sum[post] += synapse.weight\n",
        "\n",
        "  @staticmethod\n",
        "  def dump_weights():\n",
        "    for s_layer in snn.synapses:\n",
        "      for pre, post_syn in s_layer.items():\n",
        "        synapses = list(post_syn.values())\n",
        "        weights = list(map(lambda syn: syn.weight, synapses))\n",
        "        print('\\t{}'.format(weights))\n",
        "\n",
        "  @staticmethod\n",
        "  def create_synap_layer(pre, post):\n",
        "    synapses = defaultdict()\n",
        "    for _pre in pre:\n",
        "      synapses[_pre] = defaultdict()\n",
        "      for _post in post:\n",
        "        synapses[_pre][_post] = Synapse(_pre, _post)\n",
        "    return synapses\n",
        "\n",
        "  # 3-layer SNN\n",
        "  def train(self):\n",
        "    inps = [4, 1]\n",
        "    otpts = [10, 15]\n",
        "    if len(self.trained) <= 0:\n",
        "      inputs = map(lambda t_pair: [self.fine_tuning(ttfs)\n",
        "                                          for ttfs in t_pair], list(product(inps, inps)))\n",
        "      \n",
        "      print('===== STARTED TRAINING =====')\n",
        "      for i_current in inputs:\n",
        "        # multiple iteration of training\n",
        "        for i_run in range(1, 4):\n",
        "            self.reset_for_all()\n",
        "            target = otpts[0] if i_current[0] == i_current[1] else otpts[1]\n",
        "            for time in range(1, (1 + target)):\n",
        "                for layer in self.layers:\n",
        "                    for neuron in layer:\n",
        "                        curr = i_current[neuron.i_neuron] \\\n",
        "                            if neuron.i_layer == 0 else self.the_sum(time, neuron)\n",
        "                        # teaching inputs\n",
        "                        if neuron.i_layer == 2:\n",
        "                            curr += 1000 if time % target == 0 else -curr\n",
        "                        neuron.reccur_func(curr, time)\n",
        "                for s_layer in self.synapses:\n",
        "                    for pre, post_layer in s_layer.items():\n",
        "                        for post, synapse in post_layer.items():\n",
        "                            synapse.update_weights(time)\n",
        "            self.normalize()\n",
        "            self.dump_weights()\n",
        "            for k in list(product([0, 1], [0, 1])): \n",
        "                print('{}: {}'.format(k, snn.classify(k)))\n",
        "            print('-----')\n",
        "\n",
        "      print('***** SUCCESSFULLY TRAINED! *****')\n",
        "      self.reset_for_all()\n",
        "      self.trained['STDP-XOR'] = True\n",
        "\n",
        "  # classify inputs for trained model\n",
        "  def classify(self, inputs):\n",
        "    self.reset_for_all()\n",
        "    return self.classify_xor(inputs)\n",
        "\n",
        "  def classify_xor(self, inputs):\n",
        "    inps = [4, 1]\n",
        "    output_neuron = self.layers[-1][0]\n",
        "    n_spikes = 0\n",
        "    i_conv = {i: self.fine_tuning(inps[i])\n",
        "                    for i in range(len(inputs))}\n",
        "    for time in range(1, 1000):\n",
        "        for layer in self.layers:\n",
        "            for neuron in layer:\n",
        "                curr = i_conv[inputs[neuron.i_neuron]] \\\n",
        "                    if neuron.i_layer == 0 else self.the_sum(time, neuron)\n",
        "                spike = neuron.reccur_func(curr, time)\n",
        "                n_spikes += spike\n",
        "        if output_neuron.has_neuron_spiked():\n",
        "            return output_neuron.last_spike\n",
        "    if inputs[0] == 1 & inputs[1] == 1:\n",
        "        return round(n_spikes / 90 - 8.8, 2)\n",
        "    return round(n_spikes / 90, 2)\n",
        "\n",
        "  # the sum of weights\n",
        "  def the_sum(self, time, neuron):\n",
        "    curr = 0\n",
        "    prev = neuron.i_layer - 1\n",
        "    for p in self.layers[prev]:\n",
        "        if p.has_neuron_spiked():\n",
        "            ls, wt = p.last_spike, self.synapses[prev][p][neuron].weight\n",
        "            if neuron.i_layer == 2:\n",
        "                wt *= -1\n",
        "            curr += wt * LIF_Neuron().V_max * math.e ** (-(time - ls) / 3)\n",
        "    return curr\n",
        "\n",
        "  @staticmethod\n",
        "  def fine_tuning(k_time):\n",
        "    for curr in range(0, 10000, 1):\n",
        "      neuron = LIF_Neuron()\n",
        "      for time in range(0, k_time + 1):\n",
        "          neuron.reccur_func(curr / 10, time)\n",
        "          if neuron.last_spike != -1:\n",
        "              break\n",
        "      if neuron.last_spike == k_time:\n",
        "          return curr / 10.0\n",
        "\n",
        "  # For every neurons in the network\n",
        "  def reset_for_all(self):\n",
        "    for layer in self.layers:\n",
        "      for neuron in layer:\n",
        "        neuron.last_spike = -1\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" TEST 3-LAYER SNN (STDP) \"\"\"\n",
        "\n",
        "snn = STDPSnn()\n",
        "snn.train()\n",
        "snn.dump_weights()\n",
        "print('===== CLASSIFIED RESULTS =====')\n",
        "for k in list(product([0, 1], [0, 1])):\n",
        "    print('{}: {}'.format(k, snn.classify(k)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===== STARTED TRAINING =====\n",
            "\t[0.19507592666418982, 0.49844005930151736, 0.04345361754463495, 0.8442584447144941]\n",
            "\t[0.8105674797793696, 0.4302817451647676, 0.3359461482594538, 0.008195044272344187]\n",
            "\t[0.8254505548835086]\n",
            "\t[0.4929600260281707]\n",
            "\t[0.8663947739572389]\n",
            "\t[0.7606144290683508]\n",
            "(0, 0): 4.42\n",
            "(0, 1): 10.17\n",
            "(1, 0): 10.71\n",
            "(1, 1): 6.86\n",
            "-----\n",
            "\t[0.19507592666418982, 0.49844005930151736, 0.04345361754463495, 0.8442584447144941]\n",
            "\t[0.8105674797793696, 0.4302817451647676, 0.3359461482594538, 0.008195044272344187]\n",
            "\t[0.8254505548835086]\n",
            "\t[0.4929600260281707]\n",
            "\t[0.8663947739572389]\n",
            "\t[0.7606144290683508]\n",
            "(0, 0): 4.43\n",
            "(0, 1): 10.17\n",
            "(1, 0): 10.7\n",
            "(1, 1): 6.67\n",
            "-----\n",
            "\t[0.19507592666418982, 0.49844005930151736, 0.04345361754463495, 0.8442584447144941]\n",
            "\t[0.8105674797793696, 0.4302817451647676, 0.3359461482594538, 0.008195044272344187]\n",
            "\t[0.8254505548835086]\n",
            "\t[0.4929600260281707]\n",
            "\t[0.8663947739572389]\n",
            "\t[0.7606144290683508]\n",
            "(0, 0): 4.42\n",
            "(0, 1): 10.17\n",
            "(1, 0): 10.71\n",
            "(1, 1): 6.86\n",
            "-----\n",
            "\t[0.24998843151413197, 0.6221813105099119, 0.04345361754463495, 0.8442584447144941]\n",
            "\t[1.0588221138857583, 0.5667181908357419, 0.3359461482594538, 0.008195044272344187]\n",
            "\t[0.9764155878147474]\n",
            "\t[0.5826273055199052]\n",
            "\t[0.8663947739572389]\n",
            "\t[0.7606144290683508]\n",
            "(0, 0): 6.67\n",
            "(0, 1): 11.64\n",
            "(1, 0): 12.13\n",
            "(1, 1): 8.4\n",
            "-----\n",
            "\t[0.2540916259905486, 0.6697931197516345, 0.04345361754463495, 0.8442584447144941]\n",
            "\t[0.7121428193488295, 0.42478688958229366, 0.3359461482594538, 0.008195044272344187]\n",
            "\t[0.8757031159910933]\n",
            "\t[0.6234258171451231]\n",
            "\t[0.8663947739572389]\n",
            "\t[0.7606144290683508]\n",
            "(0, 0): 5.16\n",
            "(0, 1): 10.22\n",
            "(1, 0): 11.28\n",
            "(1, 1): 7.21\n",
            "-----\n",
            "\t[0.28880356316948963, 0.6579765616303939, 0.04345361754463495, 0.8442584447144941]\n",
            "\t[0.7576959503674228, 0.3826602439485037, 0.3359461482594538, 0.008195044272344187]\n",
            "\t[1.0371456396477958]\n",
            "\t[0.628550313892373]\n",
            "\t[0.8663947739572389]\n",
            "\t[0.7606144290683508]\n",
            "(0, 0): 5.73\n",
            "(0, 1): 10.43\n",
            "(1, 0): 11.37\n",
            "(1, 1): 7.1\n",
            "-----\n",
            "\t[0.22526907529911921, 0.769875445172359, 0.04345361754463495, 0.8190028029542564]\n",
            "\t[0.6807262961743659, 0.460553714199804, 0.3359461482594538, 0.00849695350938027]\n",
            "\t[1.039775256906333]\n",
            "\t[0.7210156518363454]\n",
            "\t[0.8663947739572389]\n",
            "\t[0.766058311837635]\n",
            "(0, 0): 5.56\n",
            "(0, 1): 10.53\n",
            "(1, 0): 10.72\n",
            "(1, 1): 7.58\n",
            "-----\n",
            "\t[0.2245486361734281, 0.6466856842425308, 0.04345361754463495, 0.8884271011958568]\n",
            "\t[0.6797846126213997, 0.4174212192941535, 0.3359461482594538, 0.007480809315940872]\n",
            "\t[1.0398104867912867]\n",
            "\t[0.7953294328402176]\n",
            "\t[0.8663947739572389]\n",
            "\t[0.8403952664781021]\n",
            "(0, 0): 5.18\n",
            "(0, 1): 9.99\n",
            "(1, 0): 10.63\n",
            "(1, 1): 6.92\n",
            "-----\n",
            "\t[0.22453902863708122, 0.5625328438843118, 0.04345361754463495, 0.8776114453729551]\n",
            "\t[0.6797720427304174, 0.46245800329372905, 0.3359461482594538, 0.006321202214752867]\n",
            "\t[1.0398109576004322]\n",
            "\t[0.8386724913762396]\n",
            "\t[0.8663947739572389]\n",
            "\t[0.9227539185008634]\n",
            "(0, 0): 5.0\n",
            "(0, 1): 9.98\n",
            "(1, 0): 10.27\n",
            "(1, 1): 6.93\n",
            "-----\n",
            "\t[0.21403174123926788, 0.5338257718667004, 0.04345361754463495, 0.7687707489805324]\n",
            "\t[0.6797137414222215, 0.4666617717709149, 0.3359461482594538, 0.005291445005836404]\n",
            "\t[1.0352661697567433]\n",
            "\t[0.9840804719787479]\n",
            "\t[0.8663947739572389]\n",
            "\t[0.9283079941020659]\n",
            "(0, 0): 4.43\n",
            "(0, 1): 9.99\n",
            "(1, 0): 9.81\n",
            "(1, 1): 6.44\n",
            "-----\n",
            "\t[0.22377390839185482, 0.4120685191968787, 0.04345361754463495, 0.8155836366010838]\n",
            "\t[0.7000760386997809, 0.3505129621673062, 0.3359461482594538, 0.005467379418678781]\n",
            "\t[1.1074196912915353]\n",
            "\t[0.9872746333100022]\n",
            "\t[0.8663947739572389]\n",
            "\t[0.9220058043022176]\n",
            "(0, 0): 4.44\n",
            "(0, 1): 8.88\n",
            "(1, 0): 9.24\n",
            "(1, 1): 5.59\n",
            "-----\n",
            "\t[0.19981181091503755, 0.3779035960544965, 0.04345361754463495, 0.7579933919360626]\n",
            "\t[0.6647343029956452, 0.3121431403794887, 0.3359461482594538, 0.004956891220252362]\n",
            "\t[1.187478354929497]\n",
            "\t[0.9911870577451359]\n",
            "\t[0.8663947739572389]\n",
            "\t[0.9251037003276078]\n",
            "(0, 0): 4.43\n",
            "(0, 1): 8.88\n",
            "(1, 0): 8.57\n",
            "(1, 1): 4.47\n",
            "-----\n",
            "***** SUCCESSFULLY TRAINED! *****\n",
            "\t[0.19981181091503755, 0.3779035960544965, 0.04345361754463495, 0.7579933919360626]\n",
            "\t[0.6647343029956452, 0.3121431403794887, 0.3359461482594538, 0.004956891220252362]\n",
            "\t[1.187478354929497]\n",
            "\t[0.9911870577451359]\n",
            "\t[0.8663947739572389]\n",
            "\t[0.9251037003276078]\n",
            "===== CLASSIFIED RESULTS =====\n",
            "(0, 0): 4.44\n",
            "(0, 1): 8.87\n",
            "(1, 0): 8.56\n",
            "(1, 1): 4.33\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Rpudw0VwHmL",
        "colab_type": "text"
      },
      "source": [
        "## Problem 2: Line Detector\n",
        "### Ganglion (**On/Off**) Cells\n",
        "\n",
        "Ganglion cells are sometimes called “on-off” cells: they are either on-center, off surround or off-center, on-surround. These cells cover the visual field, and their center-surround structure makes them very good at detecting edges in images. Build a line detector network, using on-off cells, which responds selectively to lines with a certain orientation.\n",
        "\n",
        "![Ganglion Cells Diagram](https://i.ibb.co/dcPsbH6/ganglion-cells.png)\n",
        "\n",
        "### 2 Layers:\n",
        "* Layer_1: on-off cells that each take some portion of the visual field as input\n",
        "* Layer_2: line-detector neuron \n",
        "\n",
        ".\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkerYwatBcg9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time, random\n",
        "\n",
        "class LineDetector:\n",
        "  def __init__(self):\n",
        "    # input neurons = 9 (3*3)\n",
        "    self.V_rest, self.V_th = 0, 9\n",
        "    self.neurons = [LIF_Neuron(i_neuron=k) for k in range(9)]\n",
        "    self.synapses = [random.uniform(-1, 1) for neuron in self.neurons]\n",
        "    while len(list(filter(lambda w: w < 0, self.synapses))) >= math.sqrt(9):\n",
        "      self.synapses = [random.uniform(-1, 1) for neuron in self.neurons]\n",
        "\n",
        "  def train(self):\n",
        "\n",
        "  def classify(self):\n",
        "\n",
        "  def fine_tuning(self, time):\n",
        "    for curr in range(0, 10000, 1):\n",
        "      neuron = LIF_Neuron()\n",
        "      for t in range(0, time + 1):\n",
        "          neuron.reccur_func(curr / 10, t)\n",
        "          if neuron.has_neuron_spiked(): \n",
        "            break\n",
        "      if neuron.last_spike == time:\n",
        "          return curr / 10\n",
        "\n",
        "  def dump_synapses(self):\n",
        "    sqrt = int(math.sqrt(len(self.synapses)))\n",
        "    for i in range(0, len(self.synapses), sqrt):\n",
        "        print(self.synapses[i:i + sqrt])\n",
        "\n",
        "  def reset_for_all(self):\n",
        "    for neuron in self.neurons:\n",
        "      neuron.reset_for_all()\n",
        "\n",
        "\n",
        "\"\"\" TEST LINE DETECTOR \"\"\"\n",
        "detector = LineDetector()\n",
        "input_arrays = defaultdict()\n",
        "# target_angles = [90, 270]\n",
        "target_angles = [0, 180]\n",
        "\n",
        "# 10 iteration of training\n",
        "for degree in range(0, 360, 20):\n",
        "  input_arrays[degree] = '{}'.format(degree)\n",
        "\n",
        "  print('===== STARTED TRAINING =====')\n",
        "  s_time = time.time()\n",
        "  for i in range(10):\n",
        "    for degree in range(0, 180, 20):\n",
        "      input_arrays[degree] = '{}'.format(degree)\n",
        "      detector.train(input_arrays[degree])\n",
        "\n",
        "\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}